{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7a6aeb48",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pheonix/Work/JPMC/name_matching/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import hnswlib\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import time\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dfccad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Configuration Parameters ---\n",
    "# Embedding Model\n",
    "MODEL_NAME = 'all-MiniLM-L6-v2' # A good balance of size/performance for embeddings\n",
    "                               # You could try larger models like 'all-mpnet-base-v2' for higher accuracy\n",
    "                               # but they will be slower and use more RAM.\n",
    "\n",
    "# HNSW Index Parameters\n",
    "HNSW_SPACE = 'cosine'          # Distance metric for HNSW (cosine similarity is common for embeddings)\n",
    "EF_CONSTRUCTION = 200          # HNSW build time / graph quality trade-off (higher = better graph, slower build)\n",
    "M_HNSW = 16                    # HNSW connections per node (higher = denser graph, better accuracy, more memory)\n",
    "K_SEARCH_RESULTS = 10         # Number of nearest neighbors to retrieve per search query\n",
    "SEARCH_EF = 100                # HNSW search time / accuracy trade-off (higher = more accurate, slower search)\n",
    "                               # Typically, SEARCH_EF >= K_SEARCH_RESULTS\n",
    "\n",
    "# Result Display Thresholds (for filtering what's printed to console)\n",
    "SEMANTIC_SIMILARITY_THRESHOLD = 0.4 # Minimum semantic similarity score (0.0 to 1.0) to display a match\n",
    "STRING_FUZZY_THRESHOLD = 10    # Minimum basic string similarity score (0 to 100) to display a match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "57ef9e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "company_names_sample = [\n",
    "    \"Google LLC\", \"Alphabet Inc.\", \"Google Inc.\", \"Microsoft Corporation\",\n",
    "    \"Microsft Corp.\", \"Apple Inc.\", \"Apple Computers LLC\", \"Amazon.com Inc.\",\n",
    "    \"International Business Machines\", \"IBM Corp.\", \"The Coca-Cola Company\",\n",
    "    \"PepsiCo Inc.\", \"ExxonMobil Corporation\", \"Shell Global\", \"BP p.l.c.\",\n",
    "    \"Siemens AG\", \"Bosch GmbH\", \"General Electric Co.\", \"GE Power\",\n",
    "    \"Walmart Inc.\", \"Target Corporation\", \"Costco Wholesale Corp.\", \"NVIDIA Corp.\",\n",
    "    \"Advanced Micro Devices Inc.\", \"Intel Corporation\", \"Oracle Corp.\",\n",
    "    \"SAP SE\", \"Accenture plc\", \"Deloitte Touche Tohmatsu Limited\",\n",
    "    \"PricewaterhouseCoopers LLP\", \"Ernst & Young Global Limited\",\n",
    "    \"Goldman Sachs Group Inc.\", \"JPMorgan Chase & Co.\", \"Bank of America Corp.\",\n",
    "    \"Wells Fargo & Company\", \"Johnson & Johnson\", \"Pfizer Inc.\",\n",
    "    \"Novartis AG\", \"Roche Holding AG\", \"Sanofi S.A.\", \"T-Mobile US Inc.\",\n",
    "    \"Verizon Communications Inc.\", \"AT&T Inc.\", \"Samsung Electronics Co. Ltd.\",\n",
    "    \"Sony Group Corporation\", \"LG Electronics Inc.\",\n",
    "    \"International Business Management\", \"Big Blue Tech\", \"Apple Global\",\n",
    "    \"General Motors Company\", \"Ford Motor Company\", \"Toyota Motor Corporation\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "01376542",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Preprocessing Function (Minimalist) ---\n",
    "def preprocess_name(name):\n",
    "    \"\"\"\n",
    "    Applies basic normalization to company names.\n",
    "    This version keeps it minimal as discussed, relying on embeddings for complexity.\n",
    "    \"\"\"\n",
    "    return name.lower().replace('.', '').replace(',', '').replace(';', '').replace('&', 'and').strip()\n",
    "\n",
    "# --- Basic String Similarity (for re-ranking insight) ---\n",
    "# This is a very simple string similarity. For production, consider fuzzywuzzy or rapidfuzz.\n",
    "def basic_string_similarity_score(s1, s2):\n",
    "    \"\"\"Calculates a basic character-overlap string similarity score (0-100).\"\"\"\n",
    "    s1_clean = s1.replace(' ', '').lower()\n",
    "    s2_clean = s2.replace(' ', '').lower()\n",
    "    if not s1_clean or not s2_clean: return 0.0 # Handle empty strings\n",
    "    \n",
    "    # Calculate common characters\n",
    "    common_chars = set(s1_clean) & set(s2_clean)\n",
    "    matches = len(common_chars)\n",
    "    \n",
    "    # Simple overlap ratio\n",
    "    return (matches / max(len(s1_clean), len(s2_clean))) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f9c25ece",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- HNSW Index Search Performance (all-MiniLM-L6-v2 Embeddings) ---\n",
      "\n",
      "1. Loading embedding model: all-MiniLM-L6-v2...\n",
      "   Model loaded in 1.44 seconds.\n",
      "\n",
      "2. Generating embeddings for 52 sample companies...\n",
      "   Embeddings generated in 2.72 seconds. (Dim: 384)\n"
     ]
    }
   ],
   "source": [
    "print(f\"--- HNSW Index Search Performance ({MODEL_NAME} Embeddings) ---\")\n",
    "    \n",
    "# 3.1. Load Embedding Model\n",
    "print(f\"\\n1. Loading embedding model: {MODEL_NAME}...\")\n",
    "start_time = time.time()\n",
    "try:\n",
    "    model = SentenceTransformer(MODEL_NAME)\n",
    "except Exception as e:\n",
    "    print(f\"Error loading model: {e}\")\n",
    "    print(\"Please ensure you have an internet connection or the model is cached locally.\")\n",
    "    # In a Jupyter Notebook, you might not want to sys.exit(1), but rather alert the user.\n",
    "    # For this script, we'll let it proceed, but subsequent steps might fail if the model isn't loaded.\n",
    "    model = None # Indicate model loading failure\n",
    "embedding_load_time = time.time() - start_time\n",
    "print(f\"   Model loaded in {embedding_load_time:.2f} seconds.\")\n",
    "\n",
    "if model is None:\n",
    "    print(\"\\n! MODEL FAILED TO LOAD. Cannot proceed with embedding and HNSW building.\")\n",
    "else:\n",
    "    # 3.2. Generate Embeddings for Dataset\n",
    "    print(f\"\\n2. Generating embeddings for {len(company_names_sample)} sample companies...\")\n",
    "    start_time = time.time()\n",
    "    preprocessed_names = [preprocess_name(name) for name in company_names_sample]\n",
    "    embeddings = model.encode(preprocessed_names, convert_to_numpy=True)\n",
    "    embedding_dim = embeddings.shape[1]\n",
    "    embedding_gen_time = time.time() - start_time\n",
    "    print(f\"   Embeddings generated in {embedding_gen_time:.2f} seconds. (Dim: {embedding_dim})\")\n",
    "\n",
    "    # Store original names with their IDs and preprocessed versions for lookup\n",
    "    company_data = {\n",
    "        i: {'original_name': company_names_sample[i], 'preprocessed_name': preprocessed_names[i]}\n",
    "        for i in range(len(company_names_sample))\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e2f0035d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "3. Building HNSW index with 52 elements...\n",
      "   HNSW index built in 0.00 seconds. (ef_construction=200, M=16)\n"
     ]
    }
   ],
   "source": [
    "if 'embeddings' not in locals(): # Check if embeddings were successfully created\n",
    "    print(\"\\n! EMBEDDINGS NOT GENERATED. Cannot proceed with HNSW index building.\")\n",
    "else:\n",
    "    print(f\"\\n3. Building HNSW index with {len(company_names_sample)} elements...\")\n",
    "    start_time = time.time()\n",
    "    hnsw_index = hnswlib.Index(space=HNSW_SPACE, dim=embedding_dim)\n",
    "    hnsw_index.init_index(max_elements=len(company_names_sample), ef_construction=EF_CONSTRUCTION, M=M_HNSW)\n",
    "    hnsw_index.add_items(embeddings, np.arange(len(company_names_sample), dtype=np.int32))\n",
    "    index_build_time = time.time() - start_time\n",
    "    print(f\"   HNSW index built in {index_build_time:.2f} seconds. (ef_construction={EF_CONSTRUCTION}, M={M_HNSW})\")\n",
    "    \n",
    "    hnsw_index.set_ef(SEARCH_EF) # Set ef for search operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6048af82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Ready for Interactive Search ---\n",
      "Type a company name and press Enter to search. Type 'exit' to quit.\n",
      "Searching for top 5 neighbors with ef_search=100.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/v8/3p0qlbcn7p76nfxhrgrm8d5c0000gn/T/ipykernel_85853/3752246442.py:81: RuntimeWarning: overflow encountered in scalar subtract\n",
      "  semantic_similarity = 1 - distance # Cosine similarity = 1 - cosine distance\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Search for 'apple':\n",
      "   Query embedding time: 0.3555 seconds\n",
      "   HNSW query time:      0.0002 seconds\n",
      "   Total search operation time: 0.3557 seconds\n",
      "\n",
      "   No strong matches found based on current thresholds.\n",
      "   (Consider adjusting SEMANTIC_SIMILARITY_THRESHOLD or STRING_FUZZY_THRESHOLD if expected matches aren't showing.)\n",
      "\n",
      "Search for 'motor':\n",
      "   Query embedding time: 0.0536 seconds\n",
      "   HNSW query time:      0.0001 seconds\n",
      "   Total search operation time: 0.0536 seconds\n",
      "\n",
      "   No strong matches found based on current thresholds.\n",
      "   (Consider adjusting SEMANTIC_SIMILARITY_THRESHOLD or STRING_FUZZY_THRESHOLD if expected matches aren't showing.)\n",
      "\n",
      "Search for 'general motor':\n",
      "   Query embedding time: 0.6979 seconds\n",
      "   HNSW query time:      0.0000 seconds\n",
      "   Total search operation time: 0.6979 seconds\n",
      "\n",
      "   No strong matches found based on current thresholds.\n",
      "   (Consider adjusting SEMANTIC_SIMILARITY_THRESHOLD or STRING_FUZZY_THRESHOLD if expected matches aren't showing.)\n",
      "Please enter a non-empty query.\n",
      "Please enter a non-empty query.\n",
      "Please enter a non-empty query.\n",
      "Please enter a non-empty query.\n"
     ]
    }
   ],
   "source": [
    "if 'hnsw_index' not in locals():\n",
    "    print(\"\\n! HNSW INDEX NOT BUILT. Cannot proceed with search.\")\n",
    "else:\n",
    "    print(\"\\n--- Ready for Interactive Search ---\")\n",
    "    print(f\"Type a company name and press Enter to search. Type 'exit' to quit.\")\n",
    "    print(f\"Searching for top {K_SEARCH_RESULTS} neighbors with ef_search={SEARCH_EF}.\")\n",
    "\n",
    "    loop = True\n",
    "\n",
    "    while loop:\n",
    "        try:\n",
    "            query = input(\"\\nEnter company name: \").strip()\n",
    "        except EOFError: # Handles graceful exit if running as a script and input stream ends\n",
    "            print(\"\\nEOF received. Exiting search.\")\n",
    "            loop = False\n",
    "            break\n",
    "        except KeyboardInterrupt: # Handles Ctrl+C\n",
    "            print(\"\\nKeyboardInterrupt received. Exiting search.\")\n",
    "            loop = False\n",
    "            break\n",
    "\n",
    "        if query.lower() == 'exit':\n",
    "            print(\"Exiting search. Goodbye!\")\n",
    "            loop = False\n",
    "            break\n",
    "\n",
    "        if not query:\n",
    "            print(\"Please enter a non-empty query.\")\n",
    "            continue\n",
    "\n",
    "        # --- Search Process ---\n",
    "        search_start_time = time.time()\n",
    "\n",
    "        # 5.1. Preprocess Query\n",
    "        preprocessed_query = preprocess_name(query)\n",
    "        \n",
    "        # 5.2. Embed Query\n",
    "        # Reshape to (1, -1) because hnswlib expects a 2D array for queries\n",
    "        query_embedding = model.encode(preprocessed_query, convert_to_numpy=True).reshape(1, -1)\n",
    "        query_embedding_time = time.time() - search_start_time\n",
    "\n",
    "        # 5.3. Perform HNSW Query\n",
    "        hnsw_query_start_time = time.time()\n",
    "        # hnswlib.knn_query returns distances (float64) and indices (int64) NumPy arrays\n",
    "        distances, indices = hnsw_index.knn_query(query_embedding, k=K_SEARCH_RESULTS)\n",
    "        hnsw_query_time = time.time() - hnsw_query_start_time\n",
    "        \n",
    "        total_search_time = time.time() - search_start_time\n",
    "\n",
    "        # 5.4. Display Results\n",
    "        print(f\"\\nSearch for '{query}':\")\n",
    "        print(f\"   Query embedding time: {query_embedding_time:.4f} seconds\")\n",
    "        print(f\"   HNSW query time:      {hnsw_query_time:.4f} seconds\")\n",
    "        print(f\"   Total search operation time: {total_search_time:.4f} seconds\")\n",
    "\n",
    "        found_matches = []\n",
    "        # Loop through the raw NumPy arrays returned by knn_query\n",
    "        for i in range(indices.shape[1]): # Iterate k times\n",
    "            raw_indexed_id = indices[0, i] # Get the raw ID from the NumPy array\n",
    "            distance = distances[0, i] # Get the distance for this ID\n",
    "\n",
    "            # --- FIX START ---\n",
    "            # Robustly convert to integer and filter invalid IDs.\n",
    "            # Check if it's a floating point type and very close to zero or negative,\n",
    "            # indicating it's likely a placeholder for an unfound neighbor.\n",
    "            if np.issubdtype(type(raw_indexed_id), np.floating) and (raw_indexed_id < 0.0 or abs(raw_indexed_id) < 1e-6):\n",
    "                # If it's a problematic float (e.g., very small positive or negative filler), skip it.\n",
    "                continue\n",
    "            \n",
    "            # Now, safely convert to integer.\n",
    "            try:\n",
    "                indexed_id = int(raw_indexed_id)\n",
    "            except ValueError:\n",
    "                # This catches cases where raw_indexed_id might be NaN or other non-convertible types\n",
    "                print(f\"Warning: Could not convert ID {raw_indexed_id} to integer. Skipping.\")\n",
    "                continue\n",
    "\n",
    "            # Robustly handle standard invalid indices (-1 from hnswlib) or out of bounds\n",
    "            if not (0 <= indexed_id < len(company_names_sample)):\n",
    "                continue \n",
    "            # --- FIX END ---\n",
    "\n",
    "            original_name = company_data[indexed_id]['original_name']\n",
    "            preprocessed_indexed_name = company_data[indexed_id]['preprocessed_name']\n",
    "            \n",
    "            semantic_similarity = 1 - distance # Cosine similarity = 1 - cosine distance\n",
    "\n",
    "            string_fuzzy_score = basic_string_similarity_score(preprocessed_query, preprocessed_indexed_name)\n",
    "\n",
    "            if semantic_similarity >= SEMANTIC_SIMILARITY_THRESHOLD and \\\n",
    "               string_fuzzy_score >= STRING_FUZZY_THRESHOLD:\n",
    "                found_matches.append({\n",
    "                    'original_name': original_name,\n",
    "                    'semantic_similarity': semantic_similarity,\n",
    "                    'string_fuzzy_score': string_fuzzy_score\n",
    "                })\n",
    "        \n",
    "        if found_matches:\n",
    "            # Sort by semantic similarity first, then string fuzzy score\n",
    "            found_matches.sort(key=lambda x: (x['semantic_similarity'], x['string_fuzzy_score']), reverse=True)\n",
    "            print(f\"\\n   Found Matches (Semantic Sim >= {SEMANTIC_SIMILARITY_THRESHOLD:.2f} & String Fuzzy >= {STRING_FUZZY_THRESHOLD:.1f}%):\")\n",
    "            for match in found_matches:\n",
    "                print(f\"     - {match['original_name']} (Semantic: {match['semantic_similarity']:.3f}, String: {match['string_fuzzy_score']:.1f}%)\")\n",
    "        else:\n",
    "            print(\"\\n   No strong matches found based on current thresholds.\")\n",
    "            print(\"   (Consider adjusting SEMANTIC_SIMILARITY_THRESHOLD or STRING_FUZZY_THRESHOLD if expected matches aren't showing.)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
